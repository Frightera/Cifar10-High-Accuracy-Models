# %% Necessary Things
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import backend as K

from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint, LearningRateScheduler

import numpy as np

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6, verbose=1)
early_stopper = EarlyStopping(monitor='val_loss', patience=32)
csv_logger = CSVLogger('resnet50_cifar10.csv')
md_cp = ModelCheckpoint(filepath='modelResnetAug.h5', monitor='val_loss', save_best_only=True, verbose= 1)

# load train and test dataset
def load_dataset():
	(x_train, y_train), (x_test, y_test) = cifar10.load_data()
	y_train = to_categorical(y_train)
	y_test = to_categorical(y_test)
	return x_train, y_train, x_test, y_test
 
# scale pixels
def prep_pixels(train, test):
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	return train_norm, test_norm

# %% Load Data

x_train, y_train, x_test, y_test = load_dataset()
x_train, x_test = prep_pixels(x_train, x_test)

from sklearn.model_selection import train_test_split as tts
X_val, X_test, Y_val, Y_test = tts(x_test, y_test, train_size = 0.7)

# %% Input etc

input_ = tf.keras.Input(shape=(32,32,3))

upscale = tf.keras.layers.Lambda(lambda x: tf.image.resize(x,(224,224),method=tf.image.ResizeMethod.BILINEAR))(input_)

resnet = tf.keras.applications.ResNet50(include_top= False, weights=None, input_tensor = upscale, input_shape = (224,224,3))
resnet.summary()

"""
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
"""

last_layer = resnet.get_layer('conv5_block3_out')
last_layer = last_layer.output

last_layer
"""
<KerasTensor: shape=(None, 7, 7, 2048) dtype=float32 (created by layer 'conv5_block3_out')>
"""
# Instance
x = tf.keras.layers.Flatten()(last_layer)
x = tf.keras.layers.Dropout(0.4)(x)

x = tf.keras.layers.Dense(216, activation = 'relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.Dense(128, activation = tf.keras.layers.LeakyReLU(alpha=0.2))(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.Dense(64, activation = tf.keras.layers.LeakyReLU(alpha=0.2))(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dense(10, activation = 'softmax')(x)

model = tf.keras.Model( resnet.input, x)

model.summary()
# Upscaled images cause
"""
Total params: 45,301,762
Trainable params: 45,248,082
Non-trainable params: 53,680
"""

model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])
# %% Augmention
print('Using real-time data augmentation.')
 # This will do preprocessing and realtime data augmentation:
datagen = ImageDataGenerator(
        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

datagen.fit(x_train)

iterator = datagen.flow(x_train, y_train, batch_size=64)

# Fit the model on the batches generated by datagen.flow().
history = model.fit(iterator, epochs=512, verbose=1, validation_data=(X_val, Y_val),
                        callbacks=[lr_reducer, early_stopper, csv_logger, md_cp])

# Interrupted training as it is comp. expensive. Training a little more can provide 1% or 1.5%  more acc maybe.
# It was using 12 GB GPU.
"""
Epoch 00033: val_loss did not improve from 0.31543
Epoch 34/512
782/782 [==============================] - 537s 687ms/step - loss: 0.2256 - accuracy: 0.9295 - val_loss: 0.3033 - 
                                                                val_accuracy: 0.9046
"""

acc = model.evaluate(X_test, Y_test, verbose=1) # 90.1 accuracy


# %% Last Few Epochs
"""
Epoch 00028: val_loss did not improve from 0.34222
Epoch 29/512
782/782 [==============================] - 537s 686ms/step - loss: 0.3017 - accuracy: 0.9074 - 
                                                    val_loss: 0.3670 - val_accuracy: 0.8809

Epoch 00029: val_loss did not improve from 0.34222
Epoch 30/512
782/782 [==============================] - 537s 686ms/step - loss: 0.2925 - accuracy: 0.9088 - 
                                                    val_loss: 0.3818 - val_accuracy: 0.8820

Epoch 00030: val_loss did not improve from 0.34222
Epoch 31/512
782/782 [==============================] - 537s 687ms/step - loss: 0.2822 - accuracy: 0.9110 -
                                                             val_loss: 0.3500 - val_accuracy: 0.8921

Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000639606199.

Epoch 00031: val_loss did not improve from 0.34222
Epoch 32/512
782/782 [==============================] - 537s 686ms/step - loss: 0.2471 - accuracy: 0.9232 -
                                                     val_loss: 0.3154 - val_accuracy: 0.9046

Epoch 00032: val_loss improved from 0.34222 to 0.31543, saving model to modelResnetAug.h5
Epoch 33/512
782/782 [==============================] - 538s 688ms/step - loss: 0.2348 - accuracy: 0.9283 - 
                                                    val_loss: 0.3326 - val_accuracy: 0.8969
"""





