# %% Necessary Things
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import backend as K

from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping, ModelCheckpoint, LearningRateScheduler

import numpy as np

lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6, verbose=1)
early_stopper = EarlyStopping(monitor='val_loss', patience=32)
csv_logger = CSVLogger('resnet50_cifar10.csv')
md_cp = ModelCheckpoint(filepath='modelResnetAugImagenet.h5', monitor='val_loss', save_best_only=True, verbose= 1)

# %% Data etc
# load train and test dataset
def load_dataset():
	(x_train, y_train), (x_test, y_test) = cifar10.load_data()
	y_train = to_categorical(y_train)
	y_test = to_categorical(y_test)
	return x_train, y_train, x_test, y_test
 
# scale pixels
def prep_pixels(train, test):
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	return train_norm, test_norm

x_train, y_train, x_test, y_test = load_dataset()
x_train, x_test = prep_pixels(x_train, x_test)

from sklearn.model_selection import train_test_split as tts
X_val, X_test, Y_val, Y_test = tts(x_test, y_test, train_size = 0.7)

# %% Model
input_ = tf.keras.Input(shape=(32,32,3))

upscale = tf.keras.layers.Lambda(lambda x: tf.image.resize(x,(160,160),method=tf.image.ResizeMethod.BILINEAR))(input_)

resnet = tf.keras.applications.ResNet50(include_top= False, weights='imagenet', input_tensor = upscale, input_shape = (160,160,3))
resnet.summary()

"""
Total params: 23,587,712
Trainable params: 23,534,592
Non-trainable params: 53,120
"""
last_layer = resnet.get_layer('conv3_block2_add')
last_layer = last_layer.output

last_layer # <KerasTensor: shape=(None, 20, 20, 512) dtype=float32 (created by layer 'conv3_block2_add')>

x = tf.keras.layers.Flatten()(last_layer)
x = tf.keras.layers.Dropout(0.4)(x)

x = tf.keras.layers.Dense(216, activation = 'relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.Dense(128, activation = tf.keras.layers.LeakyReLU(alpha=0.2))(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.Dense(64, activation = tf.keras.layers.LeakyReLU(alpha=0.2))(x)
x = tf.keras.layers.Dropout(0.5)(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dense(10, activation = 'softmax')(x)

model = tf.keras.Model( resnet.input, x)

model.summary()
"""
Total params: 45,170,178
Trainable params: 45,162,578
Non-trainable params: 7,600
"""

model.compile(loss='categorical_crossentropy',
              optimizer=tf.keras.optimizers.RMSprop(lr = 0.001),
              metrics=['accuracy'])

 # This will do preprocessing and realtime data augmentation:
datagen = ImageDataGenerator(
        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False)  # randomly flip images

datagen.fit(x_train)

iterator = datagen.flow(x_train, y_train, batch_size=64)

# Fit the model on the batches generated by datagen.flow().
history = model.fit(iterator, epochs=512, verbose=1, validation_data=(X_val, Y_val),
                        callbacks=[lr_reducer, early_stopper, csv_logger, md_cp])

# %% Last Few Epochs
"""
Epoch 00066: val_loss did not improve from 0.32441
Epoch 67/512
782/782 [==============================] - 148s 190ms/step - loss: 0.2916 - accuracy: 0.9095 - 
val_loss: 0.3352 - val_accuracy: 0.8979

Epoch 00067: val_loss did not improve from 0.32441
Epoch 68/512
782/782 [==============================] - 148s 189ms/step - loss: 0.2984 - accuracy: 0.9078 - 
val_loss: 0.3326 - val_accuracy: 0.8991

Epoch 00068: val_loss did not improve from 0.32441
Epoch 69/512
782/782 [==============================] - 148s 189ms/step - loss: 0.2881 - accuracy: 0.9110 - 
val_loss: 0.3298 - val_accuracy: 0.8996

Epoch 00069: val_loss did not improve from 0.32441
Epoch 70/512
782/782 [==============================] - 148s 189ms/step - loss: 0.2929 - accuracy: 0.9118 - 
val_loss: 0.3278 - val_accuracy: 0.8989

Epoch 00070: val_loss did not improve from 0.32441
Epoch 71/512
782/782 [==============================] - 148s 190ms/step - loss: 0.2932 - accuracy: 0.9098 - 
val_loss: 0.3311 - val_accuracy: 0.8987

Epoch 00071: val_loss did not improve from 0.32441
Epoch 72/512
782/782 [==============================] - 148s 189ms/step - loss: 0.2884 - accuracy: 0.9110 - 
val_loss: 0.3325 - val_accuracy: 0.8989

Epoch 00072: val_loss did not improve from 0.32441
Epoch 73/512
782/782 [==============================] - 148s 189ms/step - loss: 0.2885 - accuracy: 0.9107 - 
val_loss: 0.3329 - val_accuracy: 0.8989

Epoch 00073: val_loss did not improve from 0.32441
Epoch 74/512
782/782 [==============================] - 148s 189ms/step - loss: 0.2843 - accuracy: 0.9125 - 
val_loss: 0.3317 - val_accuracy: 0.8986
"""
acc = model.evaluate(X_test, Y_test, verbose=1) # Accuracy on testing set: 89.1%











